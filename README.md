

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

## Aim

To test and compare how different prompt patterns (broad/unstructured vs. clear/refined) affect ChatGPT’s responses across multiple scenarios. The analysis focuses on quality, accuracy, and depth of generated outputs.

---

## AI Tools Required

* ChatGPT (or equivalent LLM platform)
* Document editor for recording responses
* Evaluation rubric (quality, accuracy, depth)

---

## Explanation

### Defining Prompt Types

1. **Naïve Prompt:** Broad, unstructured, often lacking detail or clear instructions.
2. **Basic Prompt:** Clear, structured, with context and explicit instructions for the AI.

### Procedure

1. For each test scenario, design **two prompts**: naïve and basic.
2. Collect ChatGPT responses for both.
3. Compare results using evaluation criteria.

---

## Test Scenarios

### Scenario 1: Creative Story Generation

* **Naïve Prompt:** “Write a story.”
* **Basic Prompt:** “Write a short story (150–200 words) about a young engineer who builds a solar-powered car to help her village. The story should have a beginning, conflict, and resolution.”

**Observation:**

* Naïve: Generic, unstructured narrative.
* Basic: Clear context, detailed plot, structured storyline.

---

### Scenario 2: Factual Question

* **Naïve Prompt:** “Tell me about electricity.”
* **Basic Prompt:** “Explain electricity in 3–4 lines, focusing on its definition, sources, and everyday uses. Keep it simple for a high school student.”

**Observation:**

* Naïve: Overly broad, scattered response.
* Basic: Concise, accurate, and audience-specific.

---

### Scenario 3: Summarization

* **Naïve Prompt:** “Summarize AI.”
* **Basic Prompt:** “Summarize the concept of Artificial Intelligence in 5 sentences, covering definition, types, key applications, and its importance in modern technology.”

**Observation:**

* Naïve: Short, vague answer.
* Basic: Organized, comprehensive, well-structured.

---

### Scenario 4: Advice / Recommendation

* **Naïve Prompt:** “Give me advice.”
* **Basic Prompt:** “Give me 3 practical tips on how a college student can manage time effectively while preparing for exams.”

**Observation:**

* Naïve: Random and unfocused suggestions.
* Basic: Specific, actionable, and relevant.

---

## Comparative Table

| Scenario              | Naïve Prompt Response       | Basic Prompt Response                 | Quality      | Accuracy | Depth  |
| --------------------- | --------------------------- | ------------------------------------- | ------------ | -------- | ------ |
| Story Generation      | Generic story, no structure | Clear story with context & resolution | Medium       | Medium   | Medium |
| Factual Question      | Broad explanation           | Concise, accurate summary             | High (basic) | High     | High   |
| Summarization         | Vague definition            | Structured, covers all key points     | Medium       | High     | High   |
| Advice/Recommendation | General, unfocused          | Actionable, 3 clear tips              | Medium       | High     | High   |

---

## Analysis

* **Quality:** Improved consistently with structured prompts.
* **Accuracy:** Basic prompts reduced irrelevant or vague outputs.
* **Depth:** Detailed prompts elicited richer, context-aware responses.
* **Key Insight:** While naïve prompts can sometimes generate creative variety, **basic prompts consistently yield better accuracy and structure.**

---

## Summary of Findings

* Prompt clarity strongly impacts **output quality and reliability.**
* Naïve prompts lead to generic or incomplete answers.
* Basic prompts guide the AI to provide **focused, audience-specific, and actionable outputs.**
* For best results, always **include context, constraints, and expected format** in prompts.

---

# OUTPUT

* The naïve prompts produced generic, less structured, and sometimes vague responses.
* The basic prompts delivered clear, accurate, and context-rich outputs with better depth and audience focus.
* Across all test scenarios (story generation, factual Q&A, summarization, advice), basic prompts consistently outperformed naïve prompts in terms of quality, accuracy, and depth.


# RESULT: 

The prompt for the above said problem executed successfully
